{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对TF-IDF降维处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导包与读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "import random\n",
    "\n",
    "import fasttext\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取训练和预测数据\n",
    "data_path = 'data/train_second.csv'\n",
    "df = pd.read_csv(data_path,header = 0, encoding='utf8')\n",
    "\n",
    "df2 = pd.read_csv('data/train_first.csv', header = 0, encoding='utf8')\n",
    "\n",
    "df = pd.concat([df, df2], ignore_index=True)\n",
    "\n",
    "test_data_path = 'data/predict_second.csv'\n",
    "test_df = pd.read_csv(test_data_path,header = 0, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集去重\n",
    "df.drop_duplicates(subset='Discuss', keep='last',inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "stop_word = []\n",
    "stop_words_path = 'dict/stopWordList.txt'\n",
    "\n",
    "with open(stop_words_path,encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        stop_word.append(line.strip())\n",
    "stop_word.append(' ')\n",
    "# 加载情感词\n",
    "dict_path = 'dict/dict.dat'\n",
    "jieba.load_userdict(dict_path)\n",
    "# 字符串清洗，去除停用词\n",
    "def clean_str(stri):    \n",
    "    stri = re.sub(u'[\\s]+|[^\\u4e00-\\u9fa5A-Za-z]+|<br />','',stri)\n",
    "#     stri = re.sub(r'<br />|[\\s+\\.\\!\\/_\\-,$%^*(+\\\"\\']+|[+—【】！，。？、～~@#￥%……&*（）]|[0-9]+', ' ', stri)  # 正则替换\n",
    "\n",
    "    cut_str = jieba.cut(stri.strip())\n",
    "    list_str = [word for word in cut_str if word not in stop_word]\n",
    "    stri = ' '.join(list_str)\n",
    "    return stri\n",
    "\n",
    "df['Discuss'] = df['Discuss'].map(lambda x : clean_str(x))\n",
    "test_df['Discuss'] = test_df['Discuss'].map(lambda x : clean_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 空白的处理方式\n",
    "def fillnull(x):\n",
    "    if x == '':\n",
    "        return '_na_'\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df['Discuss'] = df['Discuss'].map(lambda x: fillnull(x))\n",
    "test_df['Discuss'] = test_df['Discuss'].map(lambda x: fillnull(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 辅助函数，评测函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造fasttext使用的文本\n",
    "def fasttext_data(data,label):\n",
    "    fasttext_data = []\n",
    "    for i in range(len(label)):\n",
    "        sent = data[i]+\"\\t__label__\"+str(int(label[i]))\n",
    "        fasttext_data.append(sent)\n",
    "    with open('train.txt','w') as f:\n",
    "        for data in fasttext_data:\n",
    "            f.write(data)\n",
    "            f.write('\\n')\n",
    "    return 'train.txt'\n",
    "\n",
    "# 得到预测值\n",
    "def get_predict(pred):\n",
    "    score = np.array([1,2,3,4,5])\n",
    "    pred2 = []\n",
    "    for p in pred:\n",
    "        pr = np.sum(p * score)\n",
    "        pred2.append(pr)\n",
    "    return np.array(pred2)\n",
    "\n",
    "# 评测函数\n",
    "def rmsel(true_label,pred):\n",
    "    true_label = np.array(true_label)\n",
    "    pred = np.array(pred)  \n",
    "    n = len(true_label)\n",
    "    a = true_label - pred\n",
    "    rmse = np.sqrt(np.sum(a * a)/n)\n",
    "    b = 1/(1+rmse)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 朴素贝叶斯、逻辑回归模型融合\n",
    "def lrnb_cv(model1, model2, model3, df, test_df, train_merge):\n",
    "    df = df.sample(frac=1)  # 对行做shuffle\n",
    "    df = df.reset_index(drop=True)\n",
    "#     # tf-idf向量,目前min_df=1效果最好\n",
    "#     vec = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.8,use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "#     trn_term_doc = vec.fit_transform(df['Discuss'])\n",
    "#     test_term_doc = vec.transform(test_df['Discuss'])\n",
    "#     # tf-idf降维\n",
    "#     tsvd = TruncatedSVD(n_components = 180)\n",
    "#     trn_term_doc = tsvd.fit_transform(trn_term_doc)\n",
    "#     test_term_doc = tvsd.transform(test_term_doc)\n",
    "    \n",
    "    # 取出模型，lr_model和nb_model\n",
    "    nb_model = model1\n",
    "    lr_model = model2\n",
    "    ri_model = model3\n",
    "    X = trn_term_doc_scale\n",
    "    y = df['Score'].values\n",
    "    lr_pred, nb_pred ,ri_pred= [],[],[]\n",
    "    folds = list(KFold(n_splits=5, shuffle=True, random_state=2018).split(X,y))\n",
    "    \n",
    "    for train_index, test_index in folds:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # 朴素贝叶斯训练\n",
    "        nb_model.fit(X_train, y_train)\n",
    "        pred_i = nb_model.predict_proba(X_test)\n",
    "        pred_i = get_predict(pred_i)\n",
    "        print('nb cv:', rmsel(y_test, pred_i))\n",
    "        train_merge.loc[test_index, 'nb'] = pred_i  # 将验证集nb预测值进行存储\n",
    "        train_merge.loc[test_index, 'score1'] = y_test  # 将验证集实际结果进行存储\n",
    "        # 逻辑回归训练\n",
    "        lr_model.fit(X_train, y_train)\n",
    "        pred_i = lr_model.predict_proba(X_test)\n",
    "        pred_i = get_predict(pred_i)\n",
    "        print('lr cv:', rmsel(y_test, pred_i))\n",
    "        train_merge.loc[test_index, 'lr'] = pred_i  # 将验证集lr预测值进行存储\n",
    "        train_merge.loc[test_index, 'score2'] = y_test  # 将验证集实际结果进行存储\n",
    "        # 岭回归训练\n",
    "        ri_model.fit(X_train, y_train)\n",
    "        pred_i = ri_model.predict(X_test)\n",
    "        print('ri cv:', rmsel(y_test, pred_i))\n",
    "        train_merge.loc[test_index, 'ri'] = pred_i  # 将验证集ridge预测值进行存储\n",
    "        train_merge.loc[test_index, 'score4'] = y_test  # 将验证集实际结果进行存储\n",
    "        \n",
    "        # 朴素贝叶斯预测\n",
    "        nb_predi = nb_model.predict_proba(test_term_doc_scale)\n",
    "        nb_predi = get_predict(nb_predi)\n",
    "        nb_pred.append(nb_predi)\n",
    "        # 逻辑回归预测\n",
    "        lr_predi = lr_model.predict_proba(test_term_doc_scale)\n",
    "        lr_predi = get_predict(lr_predi)\n",
    "        lr_pred.append(lr_predi)\n",
    "        # 岭回归预测\n",
    "        ri_predi = ri_model.predict(test_term_doc_scale)\n",
    "        ri_pred.append(ri_predi)\n",
    "    nb_pred = np.array(nb_pred)\n",
    "    nb_pred = np.mean(nb_pred, axis=0)\n",
    "    lr_pred = np.array(lr_pred)\n",
    "    lr_pred = np.mean(lr_pred, axis=0)\n",
    "    ri_pred = np.array(ri_pred)\n",
    "    ri_pred = np.mean(ri_pred, axis=0)\n",
    "    return nb_pred, lr_pred, ri_pred # 返回三个模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fasttext模型\n",
    "def fast_cv(df, test_df, train_merge):\n",
    "#     df = df.sample(frac=1,random_state=2018)  # 对行做shuffle\n",
    "#     df = df.reset_index(drop=True)\n",
    "    fast_pred = []\n",
    "    folds = list(KFold(n_splits=5, shuffle=True, random_state=2018).split(X, y))\n",
    "    rmsels = []\n",
    "    for train_index, test_index in folds:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        train_file = fasttext_data(X_train,y_train)\n",
    "        # fasttext训练\n",
    "        classifier = fasttext.supervised(train_file, 'model.model', lr=0.08, dim=256, word_ngrams=3,bucket=200000,loss='hs', label_prefix=\"__label__\")\n",
    "        result = classifier.predict_proba(df.loc[test_index,'Discuss'].tolist(),k=5)\n",
    "        pred = [[int(sco) * proba for sco,proba in result_i] for result_i in result]\n",
    "        pred = [sum(pred_i) for pred_i in pred]\n",
    "        print('fast cv:',rmsel(y_test,pred))\n",
    "        train_merge.loc[test_index, 'fast'] = pred  # 将验证集fasttext预测值进行存储\n",
    "        train_merge.loc[test_index, 'score3'] = y_test  # 将验证集实际结果进行存储\n",
    "        # fasttext预测\n",
    "        test_result = classifier.predict_proba(test_df['Discuss'].tolist(),k=5)\n",
    "        fast_predi = [[int(sco) * proba for sco,proba in result_i] for result_i in test_result]\n",
    "        fast_predi = [sum(pred_i) for pred_i in fast_predi]\n",
    "        fast_pred.append(fast_predi)\n",
    "        \n",
    "    fast_pred = np.array(fast_pred)\n",
    "    fast_pred = np.mean(fast_pred, axis=0)\n",
    "    return fast_pred  # 返回fasttext模型预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.8,use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "trn_term_doc = vec.fit_transform(df['Discuss'])\n",
    "test_term_doc = vec.transform(test_df['Discuss'])\n",
    "# print(type(test_term_doc))\n",
    "# tf-idf降维\n",
    "tsvd = TruncatedSVD(n_components = 180)\n",
    "tsvd.fit(trn_term_doc)\n",
    "trn_term_doc = tsvd.transform(trn_term_doc)\n",
    "test_term_doc = tsvd.transform(test_term_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 融模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB()  # 朴素贝叶斯回归\n",
    "lr_model = LogisticRegression(C=10, class_weight='balanced')  # 逻辑回归模型\n",
    "ri_model = linear_model.Ridge() # 岭回归模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "trn_term_doc_scale = min_max.fit_transform(trn_term_doc)\n",
    "test_term_doc_scale = min_max.transform(test_term_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(df), 8))\n",
    "train_merge = pd.DataFrame(data)\n",
    "train_merge.columns = ['nb','lr','fast','ri','score1','score2','score3','score4']\n",
    "nb_pred, lr_pred ,ri_pred= lrnb_cv(nb_model, lr_model, ri_model, df, test_df, train_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fasttext模型\n",
    "X = df['Discuss'].values\n",
    "y = df['Score'].values\n",
    "fast_pred = fast_cv(df, test_df, train_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建测试集\n",
    "data = np.zeros((len(test_df), 4))\n",
    "test = pd.DataFrame(data)\n",
    "feature_columns=['nb','lr', 'fast','ri']\n",
    "test.columns = ['nb','lr', 'fast','ri']\n",
    "test['nb'], test['lr'], test['fast'], test['ri'] =  nb_pred, lr_pred, fast_pred, ri_pred\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "feature_columns=['nb','lr', 'fast', 'ri']\n",
    "X = train_merge[feature_columns].values\n",
    "y = train_merge['score1'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, X, y, useTrainCV=True, early_stopping_rounds=50, cv_folds=5, printFeatureImportance=True):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X, y)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, \n",
    "                          metrics='rmse', early_stopping_rounds= early_stopping_rounds)\n",
    "        alg.set_params(n_estimators = cvresult.shape[0])\n",
    "#         print(cvresult)\n",
    "        print(cvresult.shape[0])\n",
    "    #Fit the algorithm on the data\n",
    "#     alg.fit(X, y, eval_metric='rmse')\n",
    "    \n",
    "    #Predict training set:\n",
    "#     y = alg.predict(X)\n",
    "#     dtrain_predprob = alg.predict_proba(X)[:,]\n",
    "\n",
    "    #Print model report:\n",
    "#     print(\"RMSE : %.4g\" % metrics.mean_squared_error(X, y))\n",
    "\n",
    "#     print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "\n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到学习熟虑为0.1时的理想决策树目\n",
    "import xgboost as xgb\n",
    "xgb1 = xgb.XGBRegressor(learning_rate=0.1,n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n",
    "                      objective='reg:linear', eval_metric='rmse', scale_pos_weight=1, seed=2018)\n",
    "modelfit(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "score = make_scorer(rmsel)\n",
    "params_test1 = {'max_depth': list(range(3,8,2)), 'min_child_weight': list(range(1,6,2))}\n",
    "xgb2 = xgb.XGBRegressor(learning_rate=0.1,n_estimators=110, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n",
    "                      objective='reg:linear', eval_metric='rmse', scale_pos_weight=1, seed=2018)\n",
    "gsearch1 = GridSearchCV(estimator=xgb2, param_grid=params_test1, scoring=score, cv=5)\n",
    "gsearch1.fit(X, y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "bst = xgb.XGBRegressor(learning_rate=0.01,n_estimators=1100, max_depth=5, min_child_weight=5, gamma=0, subsample=0.8,\n",
    "                       eval_metric='rmse', scale_pos_weight=1, seed=2018)\n",
    "xgb_pred = []\n",
    "folds = list(KFold(n_splits=5, shuffle=True, random_state=2018).split(X,y))\n",
    "es = []\n",
    "for tr_index, te_index in folds:\n",
    "    X_train, X_test = X[tr_index], X[te_index]\n",
    "    y_train, y_test = y[tr_index], y[te_index]\n",
    "    bst.fit(X_train, y_train)\n",
    "    y_pred = bst.predict(X_test)\n",
    "    e = rmsel(y_test, y_pred)\n",
    "    print(e)\n",
    "    \n",
    "    test_pred = bst.predict(test[feature_columns].values)\n",
    "    xgb_pred.append(test_pred)\n",
    "    es.append(e)\n",
    "print(np.mean(es, axis=0)) \n",
    " \n",
    "xgb_pred = np.array(xgb_pred)\n",
    "xgb_pred = np.mean(xgb_pred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDBT调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdbt_model = GradientBoostingRegressor(learning_rate=0.01,n_estimators=1500, min_samples_split=1000,min_samples_leaf=30,max_depth=5,max_features='auto',subsample=0.8,random_state=2018)\n",
    "\n",
    "gdbt_pred = []\n",
    "folds = list(KFold(n_splits=5, shuffle=True, random_state=2018).split(X,y))\n",
    "es = []\n",
    "for tr_index, te_index in folds:\n",
    "    X_train, X_test = X[tr_index], X[te_index]\n",
    "    y_train, y_test = y[tr_index], y[te_index]\n",
    "    gdbt_model.fit(X_train, y_train)\n",
    "    y_pred = gdbt_model.predict(X_test)\n",
    "    e = rmsel(y_test, y_pred)\n",
    "    print(e)\n",
    "    \n",
    "    test_pred = gdbt_model.predict(test[feature_columns].values)\n",
    "    gdbt_pred.append(test_pred)\n",
    "    es.append(e)\n",
    "print(np.mean(es, axis=0)) \n",
    "gdbt_pred = np.array(gdbt_pred)\n",
    "gdbt_pred = np.mean(gdbt_pred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(xgb_pred, 0.01),np.percentile(xgb_pred, 0.015),np.percentile(xgb_pred, 5),np.percentile(xgb_pred, 20),np.percentile(xgb_pred, 30),np.percentile(xgb_pred, 62),np.percentile(xgb_pred, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_pred2 = xgb_pred\n",
    "# xgb_pred1 = np.where(xgb_pred1<1.72, 1,xgb_pred1)\n",
    "xgb_pred2 = np.where((xgb_pred2>1.305)&(xgb_pred2<1.28), 2, xgb_pred2)\n",
    "xgb_pred2 = np.where((xgb_pred2<3.3)&(xgb_pred2>1.99), 3, xgb_pred2)\n",
    "xgb_pred2 = np.where((xgb_pred2<4.1)&(xgb_pred2>3.9), 4, xgb_pred2)\n",
    "xgb_pred2 = np.where(xgb_pred2>4.61, 5, xgb_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Id'] = test_df['Id']\n",
    "test[['Id', 'xgb_merge2']].to_csv('result/0326-3.csv',index=None,header =None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
